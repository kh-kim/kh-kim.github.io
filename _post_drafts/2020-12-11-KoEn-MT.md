---
title: Korean/English Machine Translation using AI-Hub Dataset
key: 20201211
tags: NaturalLanguageProcessing
category: blog
---

# AI-Hub 공개 데이터셋을 활용한 한/영 기계번역

이번 포스팅은 [AI-Hub]()에서 공개한 한국어-영어 문장쌍 코퍼스를 활용한 한/영 기계번역기를 만드는 과정을 공유하고자 합니다.

이 포스팅의 모든 내용은 [패스트캠퍼스의 온라인 자연어생성 강의](https://www.fastcampus.co.kr/data_online_dpnlg)에 수록되어 있습니다.
이 강의는 [자연어처리 초급 강의](https://www.fastcampus.co.kr/data_online_dpnlp)에 이어 자연어생성(NLG)를 주로 다루고 있습니다.
특히, 아래에서 소개할 기계번역의 사례를 통해, 자연어 생성의 이론과 실습을 총 30시간 이상에 걸쳐 다룹니다.
해당 강의 또는 이 포스팅과 같은 방법을 통해 우리는 준수한 성능의 기계번역기를 무료로 개발할 수 있습니다.
만약, 사용자가 적절한 크롤링을 통해 얻은 코퍼스까지 더한다면 더욱 좋은 기계번역기를 만들 수 있을 것입니다.

또한 저의 깃 저장소([Simple NMT](https://github.com/kh-kim/simple-nmt))에 전체 코드가 공개되어 있으니, 필요에 따라 참고하시기 바랍니다.
Simple NMT는 Sequence-to-Sequence와 Transformer를 기본 모델로 제공하며, MLE 방식의 학습 이외에도 강화학습을 활용한 MRT 방식, 그리고 Dual Supervised Learning 방식을 통해 학습할 수 있는 코드를 제공합니다.
또한, Beam-search를 지원하여 일반적인 추론방식 대비 더 높은 성능을 얻을 수 있게 합니다.

## AI-Hub 소개

## Preprocessing

이렇게 다운로드 받은 코퍼스를 이제 본격적으로 학습시키기 위한 전처리를 수행합니다.

### Cleaning

다행히 AI-Hub의 데이터들은 이미 잘 정제되어 있는 상태이므로, 굳이 별도의 정제 과정을 거칠 필요가 없습니다.
만약 웹크롤링 또는 영화/드라마 자막을 코퍼스로 삼고자 한다면, 번역에 알맞은 형태로 노이즈를 제거하거나, time-stamp를 맞추는 작업 등을 수행해줘야 할 것입니다.

### Tokenization

한국어는 말이 길어질 것 같으니, 먼저 영어의 분절(tokenization)에 대해서 먼저 소개합니다.

한국어는 교착어에 속합니다.
교착어는 어간에 접사가 붙어 문장 내에서 해당 단어의 의미와 역할이 정해지게 됩니다.
따라서 우리는 하나의 어간에 다양한 접사가 붙을 수 있기 때문에, 단순히 공백(white-space)을 delimiter로 삼는다면 너무 큰 어휘사전(vocabulary)을 갖게 될 것입니다.
따라서 어간과 접사를 분리함으로써, 어휘사전의 단어 수를 줄이고 단어에 대한 희소성을 감소시킬 수 있습니다.

더욱이 한국어의 경우에는 근대에 띄어쓰기가 도입되었기 때문에, 아직 띄어쓰기와 궁합이 좋지 않습니다. -- 여전히 중국어와 일본어에는 띄어쓰기가 존재하지 않습니다.
따라서 우리는 띄어쓰기가 불분명하더라도 의사소통을 하는데 전혀 무리가 없으며, 결과적으로 띄어쓰기가 잘 지켜지지 않는 원인이 됩니다.
즉, 우리가 수집한 데이터셋은 띄어쓰기가 중구난방일 수 있으며, 웹크롤링 등을 통해 수집한 문장이 많을수록 훨씬 더 중구난방이 될 것입니다.
그러므로 우리는 분절(tokenization)을 통해 코퍼스의 띄어쓰기를 정제(normalization)할 수 있습니다.

최신 유행을 따른다면 뒷 단락에서 소개할 서브워드 분절(Subword Segmentation)은 필수적인 요소로 자리잡고 있는데, 이러한 한국어의 언어적 특징을 무시한 채 곧바로 서브워드 분절을 적용한다면, 중구난방 띄어쓰기로 인해 서브워드 분절이 이상적으로 이루어지지 않을 것입니다.

### Split into Train/Valid/Test set

이제 이렇게 일차로 분절된 코퍼스를 정해진 비율에 따라 학습/검증/테스트의 3가지 데이터셋으로 분리합니다.
AI-Hub에서는 160만 문장쌍으로 이루어진 코퍼스를 제공합니다.
만약 테스트셋을 별도로 마련하지 않고 임의로 구성한다면 아래와 같은 비율도 괜찮은 선택이 될 수 있습니다.

|데이터셋|#Lines|
|-|-|
|학습|140만|
|검증|19.9만|
|테스트|0.1만|

아쉽게도 저는 처음에 "아무 생각 없이" 일괄적으로 "120만/20만/20만"의 구성을 하는 바람에 학습 데이터가 위의 제안보다 적게 들어갔습니다.
어떻게 보면 위의 구성보다 더 나은 비율 구성 같지만, 20만 문장의 테스트셋은 빔써치(beam-search)와 BLEU측정에서 너무 많은 시간이 소요되어 실제 테스트셋으로 활용하는 것은 사실상 불가능합니다.
따라서 제안된 위의 구성에서 테스트셋이 너무 작은 것이 흠이지만, 어차피 따로 성의있게 잘 구성된 테스트셋이 아니므로 적당히 모델 사이의 성능을 비교하기 위한 용도로 사용하고자 합니다.

### Subword Segmentation (Byte Pair Encoding, BPE)

Rich Sennrich 교수에 의해 처음 제안된 서브워드 분절(Subword Segmentation)은 압축 알고리즘인 BPE 알고리즘을 활용합니다.
중요한 점은 학습셋(training set)을 기준으로 BPE 모델이 학습되어야 한다는 것입니다.
이것도 데이터에 기반하여 통계를 작성하고 이를 바탕으로 분절을 수행하는 것이기 때문에, 학습 데이터만을 보고 분절이 수행되어야 추후에 분절 여부에 따른 정확한 성능을 측정할 수 있습니다.

본래 영어의 경우에도 라틴어의 영향을 받았기 때문에, 각기 다른 의미를 지닌 subword들이 모여 하나의 단어를 이룹니다.
한글의 경우에도 원래 한자의 영향을 받았기 때문에, 각 캐릭터가 의미를 지니고 이것들이 모여 하나의 전체 의미를 지닌 단어를 만들어냅니다.

|언어|서브워드|단어|
|-|-|-|
|영어|Con(together) + centr(center) + ate(make)|Concentrate|
|한글|집(모을 집) + 중(가운데 중)|집중|

이런 언어의 특성을 반영하여 각 언어별로 별도의 알고리즘을 적용하여 서브워드 분절기를 만드는 대신, BPE 알고리즘을 활용하여 데이터에 기반하여 서브워드 분절을 수행합니다.
다만, 앞선 단락에서 이야기 한 것처럼, 한국어의 경우에는 띄어쓰기가 워낙 중구난방이므로 Mecab을 활용한 분절 이후에 서브워드 분절을 추가로 적용합니다.
(Mecab을 활용한 분절은 생략하여도 괜찮습니다.)

### Detokenization

텍스트 분류와 같은 task와 달리, 기계번역과 같은 NLG task들은 생성된 문장을 사람이 읽고 이해할 수 있는 형태여야 합니다.
따라서 신경망에 입력되기 좋은 형태로 분절된 생성 문장들을 다시 사람이 읽기 좋은 형태로 만들어주는 작업이 필요합니다.
이러한 작업을 detokenization이라고 부르며, 이는 분절을 수행할 때 약간의 트릭을 추가함으로써 쉽게 해결될 수 있습니다.

```text
Detokenization Examples
```

## 모델 학습

[Simple NMT](https://github.com/kh-kim/simple-nmt)는 Sequence-to-Sequence 모델과 Transformer 모델을 제공합니다.
또한, 강화학습을 활용한 MRT 방식, Dual Supervised Learning (DSL) 방식을 통해 해당 모델들을 학습할 수 있도록 합니다.

### 학습 환경

저의 경우에는 MRT와 DSL에서의 적절한 hyper-parameter를 찾기 위해 힘든 과정을 거쳤는데요.
특히, 기계번역의 경우에는 학습의 결과가 1~2일은 소요되기 때문에, 빠르게 결과를 보고 튜닝할 수 없어 어려움이 많았습니다.
처음에는 1080Ti 한 대를 가지고 학습을 진행하다가, 더딘 진행속도에 결국 2080Ti를 2대 더 구입하여 학습을 진행하였습니다.
그 결과 PyTorch에서 제공하는 AMP(Automatic Mixed Precision) 기능의 효과를 체감할 수 있었는데요.
1080Ti의 경우에는 메모리 사용량 개선 뿐, AMP로 인한 속도 개선은 없었던 것에 반해, 2080Ti의 경우에는 메모리 사용량 개선 뿐만 아니라, 속도 개선까지 확인할 수 있었습니다.
결과적으로 2080Ti의 경우에는 같은 조건일 때의 1080Ti에 비해서 2배 이상의 속도 개선이 이루어질 수 있었습니다.
최종적으로는 1대의 1080Ti와 2대의 2080Ti를 갖고 학습을 진행하였습니다.

### Sequence-to-Sequence with Attention

Sequence-to-Sequence(Seq2seq)는 이전까지 별다른 연구가 진행되지 못하던 NLP 학계에 큰 파란을 일으키며, NLG의 시대를 연 장본인입니다.
특히, Attention과 함께 시너지를 내며, 순식간에 기계번역을 정복하였습니다.
Seq2seq는 크게 3가지 서브모듈로 이루어져 있습니다.

첫 번째로는 인코더입니다.
인코더는 입력 문장을 처음부터 끝까지 받아, bi-directional LSTM을 통해 문장을 하나의 벡터로 압축변환 합니다.
그리고 디코더는 인코더로부터 해당 벡터를 넘겨받아 디코딩을 시작합니다.
디코딩 작업은 \<BOS\>라는 special token이 첫 번째 입력으로 들어오면 다음 단어를 예측하기위한 출력(hidden state)을 반환합니다.
그리고 제너레이터는 이 hidden state를 입력으로 받아 softmax layer를 거쳐 각 단어별 확률 값을 반환합니다.

이를 아래와 같이 Maximum Likelihood Estimation (MLE)를 통해 학습을 진행하게 됩니다.

$$\begin{gathered}
\mathcal{D}=\{(x_i, y_i)\}_{i=1}^N \\
\text{where }x_i=\{x_{i,1},\cdots,x_{i,n}\}\text{ and }y_i=\{\text{<BOS>},y_{i,1},\cdots,y_{i,m},\text{<EOS>}\}. \\
\\
\begin{aligned}
\mathcal{L}(\theta)&=-\frac{1}{N}\sum_{i=1}^N{
  \log{P(y_i|x_i;\theta)}
} \\
&=-\frac{1}{N}\sum_{i=1}^N{
  \sum_{t=1}^{m+1}{
    \log{P(y_{i,t}|x_i,y_{i,<t};\theta)}
  }
}
\end{aligned} \\
\\
\hat{\theta}=\underset{\theta\in\Theta}{\text{argmin }}{\mathcal{L}(\theta)}
\end{gathered}$$

### Transformer

Transformer는 현재 자연어처리의 모든것이라고 해도 과언이 아닙니다.
2017년 처음 발명된 이래로, 자연어 생성 분야 뿐만 아니라 자연어 이해 분야에서도 주도적인 역할을 하고 있으며, 심지어 영상처리(Computer Vision)이나 음성인식(Automatic Speech Recognition, ASR)에서도 두각을 나타냅니다.
Simple NMT에서는 Transformer를 제공함으로써 더 나은 기계번역 성능을 쉽게 얻을 수 있도록 합니다.
또한 기존의 original paper에서 제시된 vanilla Transformer 대신에, Pre-Layer-Normalized Transformer를 직접 구현하여 제공합니다.

기존의 Transformer의 경우에는 비록 논문에서 제시된 성능은 매우 높았지만, learning-rate warm-up & decay 방식을 통해 학습을 진행해야 하고, 이는 Adam을 활용함에도 불구하고 또다시 learning-rate 튜닝을 진행해야 하는 어려움을 낳았습니다.
더욱이 이 learning-rate warm-up의 경우에는 hyper-paramter에 매우 취약하여, seq2seq를 이기는 것조차 어려움이 많았습니다. 
이에 이러한 어려움을 타파하고자 많은 연구들이 이어졌고, Pre-LN Transformer의 경우에는 learning-rate warm-up 없이 일반적인 Adam을 사용하여도 최적화가 가능하였습니다.

결과적으로 Simple NMT에서도 Transformer는 기존 Seq2seq 대비 더 빠른 학습 속도와, 훨씬 더 높은 기계번역 성능을 얻을 수 있었습니다.

### Minimum Risk Training (MRT)

Seq2seq와 Transformer와 같은 모델들은 기본적으로 MLE를 통해 학습하게 될 때, teacher-forcing이라는 방법을 사용합니다.
Teacher-forcing은 학습할 때에 디코더의 입력에 이전 time-step의 출력 대신 정답을 넣어주어 likelihood를 구하는 것입니다.
하지만 추론의 경우에는 이전 time-step의 출력을 다음 time-step의 디코더의 입력으로 넣어주며, 샘플링 기반의 generation을 하게 됩니다.
따라서 학습과 추론이 다른 방식으로 이루어진다는 단점이 있습니다.
이러한 문제는 auto-regressive 모델 학습에서 발생하는 전형적인 문제입니다.

![그림]()

따라서 우리는 비록 teacher-forcing이 훌륭하게 동작하지만, 학습과 추론 방식의 괴리를 줄이면 더욱 잘 동작하지 않을까 기대해볼 수 있습니다.
이러한 이유에서 예전부터 MLE 기반의 teacher-forcing 대신에, 추론에서 사용하는 generation 기반의 방식을 활용하기 위한 연구가 많이 시도되었습니다.

또한 기계번역과 같은 NLG task들은 보통 BLEU나 ROUGE 등의 metric을 통해 결과를 채점하는데, PPL(Cross Entropy)을 최소화 하는 것은 BLEU나 ROUGE를 최대화 하는 것과 다른 결과를 낳을 수 있습니다.
즉, 우리의 목표는 BLEU를 최대화 하는 것이지만 PPL을 최소화 하고 있는 상황이므로, 기계번역의 성능을 극대화 하지 못하고 있을 수 있다는 것입니다. -- 여기에는 BLEU가 기계번역의 품질의 꽤 정확하게 반영할 수 있다는 가정이 있습니다.

$$\begin{gathered}
\mathcal{D}=\{(x_i,y_i)\}_{i=1}^N \\
\nabla_\theta\mathcal{L}(\theta)=\nabla_\theta\Big(
  \sum_{i=1}^N{
    \log{P(\hat{y}_{i,0}|x_i)\times-\big(
      \text{reward}(\hat{y}_{i,0},y_i)-\frac{1}{K}\sum_{k=1}^K{
        \text{reward}(\hat{y}_{i,k},y_i)
      }
    \big)}
  }
\Big), \\
\text{where }\hat{y}_i\sim{P(\cdot|x_i;\theta)}. \\
\\
\theta\leftarrow\theta-\eta\cdot\nabla_\theta\mathcal{L}(\theta)
\end{gathered}$$

하지만 MRT의 경우에는 위의 수식에서 볼 수 있듯이, risk(or reward) 함수를 직접 미분할 필요가 없어, BLEU 함수를 통해 최적화를 수행할 수 있습니다.
정리해보면, MRT를 학습이 도입함으로써 우리는 아래와 같은 이점을 얻을 수 있습니다.

- 샘플링 기반 최적화를 통한 학습과 추론 방식의 괴리 최소화
- BLEU 활용을 통한 실제 번역 품질에 대한 최적화

### Dual Supervised Learning (DSL)

MRT의 경우에는 뚜렷한 단점도 존재합니다.
우선 샘플링에 기반하여 학습이 진행되므로 훨씬 비효율적입니다.
샘플링에 기반한다는 것은 exploration을 많이 한다는 것이므로, 시간이 많이 소요되는 단점이 있습니다.
또한 risk 자체는 scalar 값이므로, risk를 최소화 하기 위한 정확한 방향은 알 수 없습니다. -- 이에 반해 MLE의 경우에는 gradient가 나오므로 loss를 최소화 하기 위한 방향과 크기를 알 수 있습니다.

따라서 MRT와 달리, DSL의 경우에는 MLE의 scheme 안에서 기존의 문제들을 해결하려 합니다.
두 문장 사이의 정보는 동일하다는 번역의 특징을 활용하여, dual learning을 통해 성능을 높이고자 합니다.

$$\begin{gathered}
\mathcal{L}(\theta_{x\rightarrow{y}})=\sum_{i=1}^N{
    \Big(
        \ell\big(
            f(x^i;\theta_{x\rightarrow{y}}),y^i
        \big)
        +\lambda\mathcal{L}_\text{dual}(x^i,y^i;\theta_{x\rightarrow{y}},\theta_{y\rightarrow{x}})
    \Big)
} \\
\mathcal{L}(\theta_{y\rightarrow{x}})=\sum_{i=1}^N{
    \Big(
        \ell\big(
            f(y^i;\theta_{y\rightarrow{x}}),x^i
        \big)
        +\lambda\mathcal{L}_\text{dual}(x^i,y^i;\theta_{x\rightarrow{y}},\theta_{y\rightarrow{x}})
    \Big)
} \\
\text{where }\mathcal{L}_\text{dual}(x^i,y^i;\theta_{x\rightarrow{y}},\theta_{y\rightarrow{x}})=\Big\|
    \big(
        \log{P(y^i|x^i;\theta_{x\rightarrow{y}})+\log{\hat{P}(x^i)}}
    \big)-\big(
        \log{P(x^i|y^i;\theta_{y\rightarrow{x}})+\log{\hat{P}(y^i)}}
    \big)
\Big\|_2^2.
\end{gathered}$$

$$
\nabla_{\theta_{x\rightarrow{y}}}\mathcal{L}_\text{dual}(x^i,y^i;\theta_{x\rightarrow{y}},\theta_{y\rightarrow{x}})=\nabla_{\theta_{x\rightarrow{y}}}\Big\|
    \big(
        \log{P(y^i|x^i;\theta_{x\rightarrow{y}})+\log{\hat{P}(x^i)}}
    \big)-\big(
        \log{P(x^i|y^i;\theta_{y\rightarrow{x}})+\log{\hat{P}(y^i)}}
    \big)
\Big\|_2^2.
$$

위의 수식에서처럼 regularization term의 추가를 통해 auto-regressive 모델 학습에서 생길 수 있는 문제를 해결하고자 합니다.

### Summary

|Hyper-param|Seq2seq (MLE)|Transformer (MLE)|Seq2seq (MRT)|Seq2seq (DSL)|Transformer (DSL)|
|-|-|-|-|-|-|
|source_vocab_size| | | | | |
|target_vocab_size| | | | | |
|max_length| | | | | |
|word_vec_size| | | | | |
|hidden_size| | | | | |
|n_layers| | | | | |
|dropout| | | | | |
|batch_size| | | | | |
|n_epochs| | | | | |
|optimizer| | | | | |
|learning_rate| | | | | |
|max_grad_norm| | | | | |
|rl_n_epochs| | | | | |
|dsl_n_warmup_epochs| | | | | |
|dsl_lambda| | | | | |

## Inference

### Beam Search

## Evaluation

### BLEU

### 성능

### 실제 결과 비교

## Conclusion

## References



