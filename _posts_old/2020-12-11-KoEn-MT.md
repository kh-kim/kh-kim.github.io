---
title: Korean/English Machine Translation using AI-Hub Dataset
key: 20201211
tags: NaturalLanguageProcessing
category: blog
---

# AI-Hub 공개 데이터셋을 활용한 한/영 기계번역

이번 포스팅은 [AI-Hub]()에서 공개한 한국어-영어 문장쌍 코퍼스를 활용한 한/영 기계번역기를 만드는 과정을 공유하고자 합니다.

## 강의 소개

이 포스팅의 모든 내용은 [패스트캠퍼스의 온라인 강의]()에 수록되어 있습니다.
이 강의는 [자연어처리 초급 강의]()에 이어 자연어생성(NLG)를 주로 다루고 있습니다.
특히, 아래에서 소개할 기계번역의 사례를 통해, 자연어 생성의 이론과 실습을 총 30시간에 걸쳐 다룹니다.

## AI-Hub 소개

## Preprocessing

이렇게 다운로드 받은 코퍼스를 이제 본격적으로 학습시키기 위한 전처리를 수행합니다.

### Cleaning

다행히 AI-Hub의 데이터들은 이미 잘 정제되어 있는 상태이므로, 굳이 별도의 정제 과정을 거칠 필요가 없습니다.
만약 웹크롤링 또는 영화/드라마 자막을 코퍼스로 삼고자 한다면, 번역에 알맞은 형태로 노이즈를 제거하거나, time-stamp를 맞추는 작업 등을 수행해줘야 할 것입니다.

### Tokenization

한국어는 말이 길어질 것 같으니, 먼저 영어의 분절(tokenization)에 대해서 먼저 소개합니다.

한국어는 교착어에 속합니다.
교착어는 어간에 접사가 붙어 문장 내에서 해당 단어의 의미와 역할이 정해지게 됩니다.
따라서 우리는 하나의 어간에 다양한 접사가 붙을 수 있기 때문에, 단순히 공백(white-space)을 delimiter로 삼는다면 너무 큰 어휘사전(vocabulary)을 갖게 될 것입니다.
따라서 어간과 접사를 분리함으로써, 어휘사전의 단어 수를 줄이고 단어에 대한 희소성을 감소시킬 수 있습니다.

더욱이 한국어의 경우에는 근대에 띄어쓰기가 도입되었기 때문에, 아직 띄어쓰기와 궁합이 좋지 않습니다. -- 여전히 중국어와 일본어에는 띄어쓰기가 존재하지 않습니다.
따라서 우리는 띄어쓰기가 불분명하더라도 의사소통을 하는데 전혀 무리가 없으며, 결과적으로 띄어쓰기가 잘 지켜지지 않는 원인이 됩니다.
즉, 우리가 수집한 데이터셋은 띄어쓰기가 중구난방일 수 있으며, 웹크롤링 등을 통해 수집한 문장이 많을수록 훨씬 더 중구난방이 될 것입니다.
그러므로 우리는 분절(tokenization)을 통해 코퍼스의 띄어쓰기를 정제(normalization)할 수 있습니다.

최신 유행을 따른다면 뒷 단락에서 소개할 서브워드 분절(Subword Segmentation)은 필수적인 요소로 자리잡고 있는데, 이러한 한국어의 언어적 특징을 무시한 채 곧바로 서브워드 분절을 적용한다면, 중구난방 띄어쓰기로 인해 서브워드 분절이 이상적으로 이루어지지 않을 것입니다.

### Split into Train/Valid/Test set

이제 이렇게 일차로 분절된 코퍼스를 정해진 비율에 따라 학습/검증/테스트의 3가지 데이터셋으로 분리합니다.
AI-Hub에서는 160만 문장쌍으로 이루어진 코퍼스를 제공합니다.
만약 테스트셋을 별도로 마련하지 않고 임의로 구성한다면 아래와 같은 비율도 괜찮은 선택이 될 수 있습니다.

|데이터셋|#Lines|
|-|-|
|학습|140만|
|검증|19.9만|
|테스트|0.1만|

아쉽게도 저는 처음에 "아무 생각 없이" 일괄적으로 "120만/20만/20만"의 구성을 하는 바람에 학습 데이터가 위의 제안보다 적게 들어갔습니다.
어떻게 보면 위의 구성보다 더 나은 비율 구성 같지만, 20만 문장의 테스트셋은 빔써치(beam-search)와 BLEU측정에서 너무 많은 시간이 소요되어 실제 테스트셋으로 활용하는 것은 사실상 불가능합니다.
따라서 제안된 위의 구성에서 테스트셋이 너무 작은 것이 흠이지만, 어차피 따로 성의있게 잘 구성된 테스트셋이 아니므로 적당히 모델 사이의 성능을 비교하기 위한 용도로 사용하고자 합니다.

### Subword Segmentation (Byte Pair Encoding, BPE)

Rich Sennrich 교수에 의해 처음 제안된 서브워드 분절(Subword Segmentation)은 압축 알고리즘인 BPE 알고리즘을 활용합니다.
중요한 점은 학습셋(training set)을 기준으로 BPE 모델이 학습되어야 한다는 것입니다.
이것도 데이터에 기반하여 통계를 작성하고 이를 바탕으로 분절을 수행하는 것이기 때문에, 학습 데이터만을 보고 분절이 수행되어야 추후에 분절 여부에 따른 정확한 성능을 측정할 수 있습니다.

본래 영어의 경우에도 라틴어의 영향을 받았기 때문에, 각기 다른 의미를 지닌 subword들이 모여 하나의 단어를 이룹니다.
한글의 경우에도 원래 한자의 영향을 받았기 때문에, 각 캐릭터가 의미를 지니고 이것들이 모여 하나의 전체 의미를 지닌 단어를 만들어냅니다.

|언어|서브워드|단어|
|-|-|-|
|영어|Con(together) + centr(center) + ate(make)|Concentrate|
|한글|집(모을 집) + 중(가운데 중)|집중|

이런 언어의 특성을 반영하여 각 언어별로 별도의 알고리즘을 적용하여 서브워드 분절기를 만드는 대신, BPE 알고리즘을 활용하여 데이터에 기반하여 서브워드 분절을 수행합니다.
다만, 앞선 단락에서 이야기 한 것처럼, 한국어의 경우에는 띄어쓰기가 워낙 중구난방이므로 Mecab을 활용한 분절 이후에 서브워드 분절을 추가로 적용합니다.
(Mecab을 활용한 분절은 생략하여도 괜찮습니다.)

### Detokenization

텍스트 분류와 같은 task와 달리, 기계번역과 같은 NLG task들은 생성된 문장을 사람이 읽고 이해할 수 있는 형태여야 합니다.
따라서 신경망에 입력되기 좋은 형태로 분절된 생성 문장들을 다시 사람이 읽기 좋은 형태로 만들어주는 작업이 필요합니다.
이러한 작업을 detokenization이라고 부르며, 이는 분절을 수행할 때 약간의 트릭을 추가함으로써 쉽게 해결될 수 있습니다.

```text
Detokenization Examples
```

### TorchText

## 모델 학습

### 학습 환경

### Sequence-to-Sequence with Attention

### Transformer

#### How to optimize Transformer

### Minimum Risk Training (MRT)

### Dual Supervised Learning (DSL)

### Summary

|Hyper-param|Seq2seq (MLE)|Transformer (MLE)|Seq2seq (MRT)|Seq2seq (DSL)|Transformer (DSL)|
|-|-|-|-|-|-|
|batch_size| | | | | |
|n_epochs| | | | | |
|rl_n_epochs| | | | | |
|optimizer| | | | | |
|LR| | | | | |
|max_length| | | | | |
|dropout| | | | | |
|word_vec_size| | | | | |
|hidden_size| | | | | |
|n_layers| | | | | |
|max_grad_norm| | | | | |
|dsl_n_warmup_epochs| | | | | |
|dsl_lambda| | | | | |

## Inference

### Beam Search

## Evaluation

### BLEU

### 성능

### 실제 결과 비교

## Conclusion

## References



